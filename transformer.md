Attention mechanism: concept that let the model dynamically focus on the most revelant parts of the input sequence.

Transformer Architectures in NLP

It can be broken down into two main blocks, Encoder and decoder

Three fundamental transformer architecture: encoder-decoder, encoder-only, decoder-only

Encoder-decoder is used when for example the model must take an entire input sentence in one language and generate a new output in another(translation)
